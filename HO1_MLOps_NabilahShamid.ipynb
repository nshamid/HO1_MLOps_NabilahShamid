{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nshamid/HO1_MLOps_NabilahShamid/blob/main/HO1_MLOps_NabilahShamid.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Source"
      ],
      "metadata": {
        "id": "odmJWCXJpByS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "2DordBA8hutL"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset diunggah di Github untuk kemudahan akses\n",
        "dataset_url = 'https://github.com/nshamid/HO1_MLOps_NabilahShamid/raw/refs/heads/main/data_scientist_jobstreet_scraped_v2.zip'\n",
        "\n",
        "# Unduh file ZIP\n",
        "response = requests.get(dataset_url)\n",
        "with open('dataset.zip', 'wb') as f:\n",
        "    f.write(response.content)\n",
        "\n",
        "print(\"ZIP file berhasil diunduh!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wp0YPnBChvre",
        "outputId": "464ce4b7-ff8f-4a08-8da9-22dc048e7cb8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ZIP file berhasil diunduh!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengecek nama file\n",
        "with zipfile.ZipFile('dataset.zip', 'r') as zip_ref:\n",
        "    file_list = zip_ref.namelist()\n",
        "    print(\"Isi file di dalam ZIP:\")\n",
        "    for f in file_list:\n",
        "        print(f)\n",
        "\n",
        "    # Ekstrak semua file\n",
        "    zip_ref.extractall()\n",
        "\n",
        "print(\"ZIP file berhasil diekstrak!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WPtsPnlixa4",
        "outputId": "57e2b497-8e30-461a-bad3-ca9316dade00"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Isi file di dalam ZIP:\n",
            "data_scientist_jobstreet_scraped_v2.csv\n",
            "ZIP file berhasil diekstrak!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = 'data_scientist_jobstreet_scraped_v2.csv'\n",
        "\n",
        "# Load CSV ke DataFrame\n",
        "df = pd.read_csv(file_name)"
      ],
      "metadata": {
        "id": "JpBY01WOi01f"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgWCRcrTjA3I",
        "outputId": "6aa23b77-bcdb-4319-8f62-10959d935617"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0    job_id                              job_title  \\\n",
            "0           0  72761527                          Data Engineer   \n",
            "1           1  72787241         Machine Learning Engineer (AI)   \n",
            "2           2  72866732               Senior Risk/Data Analyst   \n",
            "3           3  72851872  Senior Data Engineer (Hybrid Working)   \n",
            "4           4  72526811        Data Scientist (Hybrid Working)   \n",
            "\n",
            "                             company  \\\n",
            "0          ANHSIN TECHNOLOGY SDN BHD   \n",
            "1            Accordia Global Sdn Bhd   \n",
            "2  Toyota Capital Malaysia Sdn. Bhd.   \n",
            "3                               SEEK   \n",
            "4              SEEK Asia (JobStreet)   \n",
            "\n",
            "                                        descriptions          location  \\\n",
            "0  Design, develop, and maintain scalable and rob...      Kuala Lumpur   \n",
            "1  Design, develop, and deploy machine learning m...  Shah Alam/Subang   \n",
            "2  Analyse data to better understand potential ri...          Petaling   \n",
            "3  Design, development, testing, and operation of...      Kuala Lumpur   \n",
            "4  Research, build, deploy and maintain Recommend...      Kuala Lumpur   \n",
            "\n",
            "                                 category  \\\n",
            "0                    Science & Technology   \n",
            "1                    Science & Technology   \n",
            "2            Banking & Financial Services   \n",
            "3  Information & Communication Technology   \n",
            "4  Information & Communication Technology   \n",
            "\n",
            "                                      subcategory       type  \\\n",
            "0  Mathematics, Statistics & Information Sciences  Full time   \n",
            "1  Mathematics, Statistics & Information Sciences  Full time   \n",
            "2                               Compliance & Risk  Full time   \n",
            "3                          Engineering - Software  Full time   \n",
            "4                          Developers/Programmers  Full time   \n",
            "\n",
            "                          salary  \n",
            "0                            NaN  \n",
            "1  RM 5,000 – RM 7,000 per month  \n",
            "2                            NaN  \n",
            "3                            NaN  \n",
            "4                            NaN  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLgKHDagjO_3",
        "outputId": "9156c4ca-2aec-4ed7-972e-051d04ad16a6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 606 entries, 0 to 605\n",
            "Data columns (total 10 columns):\n",
            " #   Column        Non-Null Count  Dtype \n",
            "---  ------        --------------  ----- \n",
            " 0   Unnamed: 0    606 non-null    int64 \n",
            " 1   job_id        606 non-null    int64 \n",
            " 2   job_title     606 non-null    object\n",
            " 3   company       606 non-null    object\n",
            " 4   descriptions  588 non-null    object\n",
            " 5   location      606 non-null    object\n",
            " 6   category      606 non-null    object\n",
            " 7   subcategory   606 non-null    object\n",
            " 8   type          606 non-null    object\n",
            " 9   salary        198 non-null    object\n",
            "dtypes: int64(2), object(8)\n",
            "memory usage: 47.5+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepocessing"
      ],
      "metadata": {
        "id": "goYNyjGBvqAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas nltk Sastrawi nlp-id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b7OndPss0h4",
        "outputId": "039623db-8817-40a6-855b-7b73e4aaec16"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Collecting Sastrawi\n",
            "  Downloading Sastrawi-1.0.1-py2.py3-none-any.whl.metadata (909 bytes)\n",
            "Collecting nlp-id\n",
            "  Downloading nlp_id-0.1.19.0-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Collecting huggingface-hub==0.31.1 (from nlp-id)\n",
            "  Downloading huggingface_hub-0.31.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: scikit-learn==1.6.1 in /usr/local/lib/python3.11/dist-packages (from nlp-id) (1.6.1)\n",
            "Requirement already satisfied: scipy==1.15.3 in /usr/local/lib/python3.11/dist-packages (from nlp-id) (1.15.3)\n",
            "Collecting wget==3.2 (from nlp-id)\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.31.1->nlp-id) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.31.1->nlp-id) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.31.1->nlp-id) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.31.1->nlp-id) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.31.1->nlp-id) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.31.1->nlp-id) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.31.1->nlp-id) (1.1.5)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.6.1->nlp-id) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub==0.31.1->nlp-id) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub==0.31.1->nlp-id) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub==0.31.1->nlp-id) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub==0.31.1->nlp-id) (2025.7.14)\n",
            "Downloading Sastrawi-1.0.1-py2.py3-none-any.whl (209 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.7/209.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nlp_id-0.1.19.0-py3-none-any.whl (8.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.31.1-py3-none-any.whl (484 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m484.3/484.3 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=736d9bede4102ea0f7b5a96aded08001b4475f159f1b6c5ed8cf7bff7bd98748\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/b3/0f/a40dbd1c6861731779f62cc4babcb234387e11d697df70ee97\n",
            "Successfully built wget\n",
            "Installing collected packages: wget, Sastrawi, huggingface-hub, nlp-id\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.33.4\n",
            "    Uninstalling huggingface-hub-0.33.4:\n",
            "      Successfully uninstalled huggingface-hub-0.33.4\n",
            "Successfully installed Sastrawi-1.0.1 huggingface-hub-0.31.1 nlp-id-0.1.19.0 wget-3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6V2GFdQs4DZ",
        "outputId": "469a8033-ff7c-4412-defd-62b3a939df6f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from nlp_id.lemmatizer import Lemmatizer\n",
        "\n",
        "# Inisialisasi stemmer dari Sastrawi\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "# Inisialisasi lemmatizer dari nlp_id\n",
        "lemmatizer = Lemmatizer()\n",
        "\n",
        "print(\"Library berhasil diimpor dan tools preprocessing diinisialisasi.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r97Ac1k0s5g7",
        "outputId": "33193637-a5e3-49f1-9424-31eb997409ec"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Library berhasil diimpor dan tools preprocessing diinisialisasi.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Menerapkan prepoccessing pada kolom\n",
        "1. job_title (Judul Pekerjaan)\n",
        "\n",
        "2. descriptions (Deskripsi Pekerjaan)"
      ],
      "metadata": {
        "id": "0jvgztZuijCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_noise(text):\n",
        "  # Menghapus semua tag HTML\n",
        "  text = re.sub(r'<.*?>', '', text)\n",
        "  # Menghapus URL\n",
        "  text = re.sub(r'https?://\\\\S+|www\\\\.\\\\S+', '', text)\n",
        "  # Menghapus Hashtag\n",
        "  text = re.sub(r'#\\\\w+', '', text)\n",
        "  # Menggunakan re.sub(r'[^\\w\\s]', '', text)\n",
        "  text = re.sub(r'[^\\w\\s]', '', text)\n",
        "  # Menghapus Angka\n",
        "  text = re.sub(r'\\\\d+', '', text)\n",
        "  # Menghapus spasi berlebih\n",
        "  text = re.sub(r'\\\\s+', ' ', text).strip()\n",
        "  return text\n",
        "\n",
        "def remove_stopwords(text):\n",
        "  list_stopwords = stopwords.words('indonesian')\n",
        "  tokens = text.split()\n",
        "  tokens_without_stopwords = [word for word in tokens if word not in list_stopwords]\n",
        "  text = ' '.join(tokens_without_stopwords)\n",
        "  return text\n",
        "\n",
        "def cleaning_pipeline(text):\n",
        "  if not isinstance(text, str):\n",
        "      return text\n",
        "\n",
        "  text = text.lower() # 1. Lowercasing\n",
        "  text = clean_noise(text) # 2. Menghapus Noise\n",
        "  text = remove_stopwords(text) # 3. Menghapus Stopwords\n",
        "  text = stemmer.stem(text) # 4. Stemming\n",
        "  return text\n",
        "\n",
        "def normalize_categorical_string(text):\n",
        "  if not isinstance(text, str):\n",
        "      return text\n",
        "  return text.lower().strip()\n",
        "\n",
        "print(\"Fungsi-fungsi preprocessing telah didefinisikan (termasuk perbaikan regex untuk spasi).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzTmIoabs9Vw",
        "outputId": "229bc807-1ec6-4ab4-b2c7-5e0dc2a8f575"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fungsi-fungsi preprocessing telah didefinisikan (termasuk perbaikan regex untuk spasi).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Muat dataset dari file CSV\n",
        "try:\n",
        "    import pandas as pd\n",
        "\n",
        "    # Membaca file CSV\n",
        "    df = pd.read_csv('data_scientist_jobstreet_scraped_v2.csv')\n",
        "\n",
        "    # Menghapus kolom 'Unnamed'\n",
        "    if 'Unnamed: 0' in df.columns:\n",
        "        df = df.drop('Unnamed: 0', axis=1)\n",
        "        print(\"\\nKolom 'Unnamed: 0' berhasil dihapus.\")\n",
        "\n",
        "    print(\"\\nMenerapkan cleaning pipeline (full cleaning) ke kolom teks bebas:\")\n",
        "\n",
        "    print(\"- Kolom 'job_title'...\")\n",
        "    df['job_title'] = df['job_title'].apply(cleaning_pipeline)\n",
        "\n",
        "    print(\"- Kolom 'descriptions'...\")\n",
        "    df['descriptions'] = df['descriptions'].apply(cleaning_pipeline)\n",
        "\n",
        "    print(\"\\nMenerapkan normalisasi ringan ke kolom string kategori:\")\n",
        "    columns_to_normalize_lightly = ['company', 'location', 'category', 'subcategory', 'type']\n",
        "    for col in columns_to_normalize_lightly:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].apply(normalize_categorical_string)\n",
        "            print(f\"- Kolom '{col}' berhasil dinormalisasi langsung ke nama kolom aslinya.\")\n",
        "        else:\n",
        "            print(f\"- Peringatan: Kolom '{col}' tidak ditemukan.\")\n",
        "\n",
        "    print(\"\\nDataFrame head setelah pembersihan dan normalisasi:\")\n",
        "    print(df.head())\n",
        "\n",
        "    print(\"\\nInfo DataFrame setelah pembaruan:\")\n",
        "    print(df.info())\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: File 'data_scientist_jobstreet_scraped_v2.csv' tidak ditemukan. Pastikan file sudah diunggah.\")\n",
        "except KeyError as e:\n",
        "    print(f\"Error: Kolom {e} tidak ditemukan di DataFrame. Pastikan nama kolom sudah benar.\")\n",
        "except Exception as e:\n",
        "    print(f\"Terjadi kesalahan saat memuat atau memproses data: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJP1xozPtA-U",
        "outputId": "1a620650-6c87-4c6d-cf1d-6cd2828d96e3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Kolom 'Unnamed: 0' berhasil dihapus.\n",
            "\n",
            "Menerapkan cleaning pipeline (full cleaning) ke kolom teks bebas:\n",
            "- Kolom 'job_title'...\n",
            "- Kolom 'descriptions'...\n",
            "\n",
            "Menerapkan normalisasi ringan ke kolom string kategori:\n",
            "- Kolom 'company' berhasil dinormalisasi langsung ke nama kolom aslinya.\n",
            "- Kolom 'location' berhasil dinormalisasi langsung ke nama kolom aslinya.\n",
            "- Kolom 'category' berhasil dinormalisasi langsung ke nama kolom aslinya.\n",
            "- Kolom 'subcategory' berhasil dinormalisasi langsung ke nama kolom aslinya.\n",
            "- Kolom 'type' berhasil dinormalisasi langsung ke nama kolom aslinya.\n",
            "\n",
            "DataFrame head setelah pembersihan dan normalisasi:\n",
            "     job_id                            job_title  \\\n",
            "0  72761527                        data engineer   \n",
            "1  72787241         machine learning engineer ai   \n",
            "2  72866732              senior riskdata analyst   \n",
            "3  72851872  senior data engineer hybrid working   \n",
            "4  72526811        data scientist hybrid working   \n",
            "\n",
            "                             company  \\\n",
            "0          anhsin technology sdn bhd   \n",
            "1            accordia global sdn bhd   \n",
            "2  toyota capital malaysia sdn. bhd.   \n",
            "3                               seek   \n",
            "4              seek asia (jobstreet)   \n",
            "\n",
            "                                        descriptions          location  \\\n",
            "0  design develop and maintain scalable and robus...      kuala lumpur   \n",
            "1  design develop and deploy machine learning mod...  shah alam/subang   \n",
            "2  analyse data to better understand potential ri...          petaling   \n",
            "3  design development testing and operation of bi...      kuala lumpur   \n",
            "4  research build deploy and maintain recommendat...      kuala lumpur   \n",
            "\n",
            "                                 category  \\\n",
            "0                    science & technology   \n",
            "1                    science & technology   \n",
            "2            banking & financial services   \n",
            "3  information & communication technology   \n",
            "4  information & communication technology   \n",
            "\n",
            "                                      subcategory       type  \\\n",
            "0  mathematics, statistics & information sciences  full time   \n",
            "1  mathematics, statistics & information sciences  full time   \n",
            "2                               compliance & risk  full time   \n",
            "3                          engineering - software  full time   \n",
            "4                          developers/programmers  full time   \n",
            "\n",
            "                          salary  \n",
            "0                            NaN  \n",
            "1  RM 5,000 – RM 7,000 per month  \n",
            "2                            NaN  \n",
            "3                            NaN  \n",
            "4                            NaN  \n",
            "\n",
            "Info DataFrame setelah pembaruan:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 606 entries, 0 to 605\n",
            "Data columns (total 9 columns):\n",
            " #   Column        Non-Null Count  Dtype \n",
            "---  ------        --------------  ----- \n",
            " 0   job_id        606 non-null    int64 \n",
            " 1   job_title     606 non-null    object\n",
            " 2   company       606 non-null    object\n",
            " 3   descriptions  588 non-null    object\n",
            " 4   location      606 non-null    object\n",
            " 5   category      606 non-null    object\n",
            " 6   subcategory   606 non-null    object\n",
            " 7   type          606 non-null    object\n",
            " 8   salary        198 non-null    object\n",
            "dtypes: int64(1), object(8)\n",
            "memory usage: 42.7+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if 'df' in locals() and not df.empty:\n",
        "    # Simpan ke CSV\n",
        "    csv_filename = 'cleaned_job_data.csv'\n",
        "    df.to_csv(csv_filename, index=False)\n",
        "    print(f\"\\nData bersih berhasil disimpan ke file CSV: '{csv_filename}'\")\n",
        "\n",
        "    # Simpan ke JSON\n",
        "    json_filename = 'cleaned_job_data.json'\n",
        "    df.to_json(json_filename, orient='records', indent=4)\n",
        "    print(f\"Data bersih berhasil disimpan ke file JSON: '{json_filename}'\")\n",
        "else:\n",
        "    print(\"DataFrame kosong atau tidak ditemukan. Pastikan langkah memuat dan memproses data berhasil.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8eRSSsjtE-S",
        "outputId": "34349fed-cbf8-4ed1-a1a2-5c5d5cfeeb85"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Data bersih berhasil disimpan ke file CSV: 'cleaned_job_data.csv'\n",
            "Data bersih berhasil disimpan ke file JSON: 'cleaned_job_data.json'\n"
          ]
        }
      ]
    }
  ]
}