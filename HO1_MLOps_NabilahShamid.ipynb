{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNLr9iMztL8QYP9WM8D0rpl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nshamid/HO1_MLOps_NabilahShamid/blob/main/HO1_MLOps_NabilahShamid.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Source"
      ],
      "metadata": {
        "id": "odmJWCXJpByS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "2DordBA8hutL"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset diunggah di Github untuk kemudahan akses\n",
        "dataset_url = 'https://github.com/nshamid/HO1_MLOps_NabilahShamid/raw/refs/heads/main/data_scientist_jobstreet_scraped_v2.zip'\n",
        "\n",
        "# Unduh file ZIP\n",
        "response = requests.get(dataset_url)\n",
        "with open('dataset.zip', 'wb') as f:\n",
        "    f.write(response.content)\n",
        "\n",
        "print(\"ZIP file berhasil diunduh!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wp0YPnBChvre",
        "outputId": "87876762-960c-4c24-abd5-22b4ee4f21c4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ZIP file berhasil diunduh!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengecek nama file\n",
        "with zipfile.ZipFile('dataset.zip', 'r') as zip_ref:\n",
        "    file_list = zip_ref.namelist()\n",
        "    print(\"Isi file di dalam ZIP:\")\n",
        "    for f in file_list:\n",
        "        print(f)\n",
        "\n",
        "    # Ekstrak semua file\n",
        "    zip_ref.extractall()\n",
        "\n",
        "print(\"ZIP file berhasil diekstrak!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WPtsPnlixa4",
        "outputId": "d1cd3dbc-e11d-4cce-adae-cadfe26f4486"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Isi file di dalam ZIP:\n",
            "data_scientist_jobstreet_scraped_v2.csv\n",
            "ZIP file berhasil diekstrak!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = 'data_scientist_jobstreet_scraped_v2.csv'\n",
        "\n",
        "# Load CSV ke DataFrame\n",
        "df = pd.read_csv(file_name)"
      ],
      "metadata": {
        "id": "JpBY01WOi01f"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgWCRcrTjA3I",
        "outputId": "cc2afab0-e846-43a1-e59d-701bc42c2bcc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0    job_id                              job_title  \\\n",
            "0           0  72761527                          Data Engineer   \n",
            "1           1  72787241         Machine Learning Engineer (AI)   \n",
            "2           2  72866732               Senior Risk/Data Analyst   \n",
            "3           3  72851872  Senior Data Engineer (Hybrid Working)   \n",
            "4           4  72526811        Data Scientist (Hybrid Working)   \n",
            "\n",
            "                             company  \\\n",
            "0          ANHSIN TECHNOLOGY SDN BHD   \n",
            "1            Accordia Global Sdn Bhd   \n",
            "2  Toyota Capital Malaysia Sdn. Bhd.   \n",
            "3                               SEEK   \n",
            "4              SEEK Asia (JobStreet)   \n",
            "\n",
            "                                        descriptions          location  \\\n",
            "0  Design, develop, and maintain scalable and rob...      Kuala Lumpur   \n",
            "1  Design, develop, and deploy machine learning m...  Shah Alam/Subang   \n",
            "2  Analyse data to better understand potential ri...          Petaling   \n",
            "3  Design, development, testing, and operation of...      Kuala Lumpur   \n",
            "4  Research, build, deploy and maintain Recommend...      Kuala Lumpur   \n",
            "\n",
            "                                 category  \\\n",
            "0                    Science & Technology   \n",
            "1                    Science & Technology   \n",
            "2            Banking & Financial Services   \n",
            "3  Information & Communication Technology   \n",
            "4  Information & Communication Technology   \n",
            "\n",
            "                                      subcategory       type  \\\n",
            "0  Mathematics, Statistics & Information Sciences  Full time   \n",
            "1  Mathematics, Statistics & Information Sciences  Full time   \n",
            "2                               Compliance & Risk  Full time   \n",
            "3                          Engineering - Software  Full time   \n",
            "4                          Developers/Programmers  Full time   \n",
            "\n",
            "                          salary  \n",
            "0                            NaN  \n",
            "1  RM 5,000 – RM 7,000 per month  \n",
            "2                            NaN  \n",
            "3                            NaN  \n",
            "4                            NaN  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLgKHDagjO_3",
        "outputId": "08ae6061-ec20-4110-e541-e0b2a0320c92"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 606 entries, 0 to 605\n",
            "Data columns (total 11 columns):\n",
            " #   Column             Non-Null Count  Dtype \n",
            "---  ------             --------------  ----- \n",
            " 0   Unnamed: 0         606 non-null    int64 \n",
            " 1   job_id             606 non-null    int64 \n",
            " 2   job_title          606 non-null    object\n",
            " 3   company            606 non-null    object\n",
            " 4   descriptions       588 non-null    object\n",
            " 5   location           606 non-null    object\n",
            " 6   category           606 non-null    object\n",
            " 7   subcategory        606 non-null    object\n",
            " 8   type               606 non-null    object\n",
            " 9   salary             198 non-null    object\n",
            " 10  cleaned_job_title  606 non-null    object\n",
            "dtypes: int64(2), object(9)\n",
            "memory usage: 52.2+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepocessing"
      ],
      "metadata": {
        "id": "goYNyjGBvqAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas nltk Sastrawi nlp-id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b7OndPss0h4",
        "outputId": "d5ff355a-0940-4f26-9e68-a23cffecef58"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Collecting Sastrawi\n",
            "  Downloading Sastrawi-1.0.1-py2.py3-none-any.whl.metadata (909 bytes)\n",
            "Collecting nlp-id\n",
            "  Downloading nlp_id-0.1.19.0-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Collecting huggingface-hub==0.31.1 (from nlp-id)\n",
            "  Downloading huggingface_hub-0.31.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: scikit-learn==1.6.1 in /usr/local/lib/python3.11/dist-packages (from nlp-id) (1.6.1)\n",
            "Requirement already satisfied: scipy==1.15.3 in /usr/local/lib/python3.11/dist-packages (from nlp-id) (1.15.3)\n",
            "Collecting wget==3.2 (from nlp-id)\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.31.1->nlp-id) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.31.1->nlp-id) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.31.1->nlp-id) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.31.1->nlp-id) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.31.1->nlp-id) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.31.1->nlp-id) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.31.1->nlp-id) (1.1.5)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.6.1->nlp-id) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub==0.31.1->nlp-id) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub==0.31.1->nlp-id) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub==0.31.1->nlp-id) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub==0.31.1->nlp-id) (2025.7.9)\n",
            "Downloading Sastrawi-1.0.1-py2.py3-none-any.whl (209 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.7/209.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nlp_id-0.1.19.0-py3-none-any.whl (8.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m88.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.31.1-py3-none-any.whl (484 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m484.3/484.3 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=2a36ef85c666612eb4f77640736d4603b9eda0234e2afdfb4b50039d7ae41a40\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/b3/0f/a40dbd1c6861731779f62cc4babcb234387e11d697df70ee97\n",
            "Successfully built wget\n",
            "Installing collected packages: wget, Sastrawi, huggingface-hub, nlp-id\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.33.2\n",
            "    Uninstalling huggingface-hub-0.33.2:\n",
            "      Successfully uninstalled huggingface-hub-0.33.2\n",
            "Successfully installed Sastrawi-1.0.1 huggingface-hub-0.31.1 nlp-id-0.1.19.0 wget-3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6V2GFdQs4DZ",
        "outputId": "a8f04049-ec21-42c1-dac3-c3f5b85a6f1a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from nlp_id.lemmatizer import Lemmatizer\n",
        "\n",
        "# Inisialisasi stemmer dari Sastrawi\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "# Inisialisasi lemmatizer dari nlp_id\n",
        "lemmatizer = Lemmatizer()\n",
        "\n",
        "print(\"Library berhasil diimpor dan tools preprocessing diinisialisasi.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r97Ac1k0s5g7",
        "outputId": "e62a2758-fcb9-48db-d832-a8e9f9337aa7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Library berhasil diimpor dan tools preprocessing diinisialisasi.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Menerapkan prepoccessing pada kolom\n",
        "1. job_title (Judul Pekerjaan)\n",
        "\n",
        "2. descriptions (Deskripsi Pekerjaan)"
      ],
      "metadata": {
        "id": "0jvgztZuijCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_noise(text):\n",
        "  # Menghapus semua tag HTML\n",
        "  text = re.sub(r'<.*?>', '', text)\n",
        "  # Menghapus URL\n",
        "  text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "  # Menghapus Hashtag\n",
        "  text = re.sub(r'#\\w+', '', text)\n",
        "  # Menghapus Emoji dan Tanda Baca (kecuali karakter alfanumerik dan spasi)\n",
        "  text = re.sub(r'[^\\\\w\\\\s]', '', text)\n",
        "  # Menghapus Angka\n",
        "  text = re.sub(r'\\\\d+', '', text)\n",
        "  # Menghapus spasi berlebih\n",
        "  text = re.sub(r'\\\\s+', ' ', text).strip()\n",
        "  return text\n",
        "\n",
        "def remove_stopwords(text):\n",
        "  list_stopwords = stopwords.words('indonesian')\n",
        "  tokens = text.split()\n",
        "  tokens_without_stopwords = [word for word in tokens if word not in list_stopwords]\n",
        "  text = ' '.join(tokens_without_stopwords)\n",
        "  return text\n",
        "\n",
        "def cleaning_pipeline(text):\n",
        "  # Memastikan input adalah string sebelum diproses\n",
        "  if not isinstance(text, str):\n",
        "      return text\n",
        "\n",
        "  text = text.lower() # 1. Lowercasing\n",
        "  text = clean_noise(text) # 2. Menghapus Noise (URL, Hashtag, Emoji, Angka, Tanda Baca)\n",
        "  text = remove_stopwords(text) # 3. Menghapus Stopwords\n",
        "  text = stemmer.stem(text) # 4. Stemming\n",
        "  text = lemmatizer.lemmatize(text) # 5. Lemmatization\n",
        "  return text\n",
        "\n",
        "print(\"Fungsi-fungsi preprocessing telah didefinisikan.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzTmIoabs9Vw",
        "outputId": "e327ecaa-05f0-4966-fe86-6012fcfc159f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fungsi-fungsi preprocessing telah didefinisikan.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Muat dataset dari file CSV\n",
        "try:\n",
        "    df = pd.read_csv('data_scientist_jobstreet_scraped_v2.csv')\n",
        "    print(\"Dataset 'data_scientist_jobstreet_scraped_v2.csv' berhasil dimuat.\")\n",
        "\n",
        "    # Tampilkan 5 baris pertama dan informasi DataFrame untuk inspeksi awal\n",
        "    print(\"\\nOriginal DataFrame head:\")\n",
        "    print(df.head())\n",
        "    print(\"\\nOriginal DataFrame info:\")\n",
        "    print(df.info())\n",
        "\n",
        "    # Terapkan cleaning pipeline ke kolom-kolom teks yang relevan\n",
        "    # Membuat kolom baru untuk menyimpan hasil teks yang sudah dibersihkan\n",
        "\n",
        "    print(\"\\nMenerapkan cleaning pipeline ke kolom 'job_title'\")\n",
        "    df['cleaned_job_title'] = df['job_title'].apply(cleaning_pipeline)\n",
        "\n",
        "    print(\"Menerapkan cleaning pipeline ke kolom 'job_description'\")\n",
        "    df['cleaned_description'] = df['descriptions'].apply(cleaning_pipeline)\n",
        "\n",
        "    # Tampilkan beberapa baris DataFrame dengan kolom yang sudah dibersihkan\n",
        "    print(\"\\nDataFrame head setelah pembersihan teks (kolom relevan):\")\n",
        "    print(df[['job_title', 'cleaned_job_title', 'descriptions', 'cleaned_descriptions']].head())\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: File 'data_scientist_jobstreet_scraped_v2.csv' tidak ditemukan. Pastikan file sudah diunggah.\")\n",
        "except KeyError as e:\n",
        "    print(f\"Error: Kolom {e} tidak ditemukan di DataFrame. Pastikan nama kolom sudah benar.\")\n",
        "except Exception as e:\n",
        "    print(f\"Terjadi kesalahan saat memuat atau memproses data: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJP1xozPtA-U",
        "outputId": "94e8ee5b-bd58-49ba-9f9a-0fbaa9966181"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset 'data_scientist_jobstreet_scraped_v2.csv' berhasil dimuat.\n",
            "\n",
            "Original DataFrame head:\n",
            "   Unnamed: 0    job_id                              job_title  \\\n",
            "0           0  72761527                          Data Engineer   \n",
            "1           1  72787241         Machine Learning Engineer (AI)   \n",
            "2           2  72866732               Senior Risk/Data Analyst   \n",
            "3           3  72851872  Senior Data Engineer (Hybrid Working)   \n",
            "4           4  72526811        Data Scientist (Hybrid Working)   \n",
            "\n",
            "                             company  \\\n",
            "0          ANHSIN TECHNOLOGY SDN BHD   \n",
            "1            Accordia Global Sdn Bhd   \n",
            "2  Toyota Capital Malaysia Sdn. Bhd.   \n",
            "3                               SEEK   \n",
            "4              SEEK Asia (JobStreet)   \n",
            "\n",
            "                                        descriptions          location  \\\n",
            "0  Design, develop, and maintain scalable and rob...      Kuala Lumpur   \n",
            "1  Design, develop, and deploy machine learning m...  Shah Alam/Subang   \n",
            "2  Analyse data to better understand potential ri...          Petaling   \n",
            "3  Design, development, testing, and operation of...      Kuala Lumpur   \n",
            "4  Research, build, deploy and maintain Recommend...      Kuala Lumpur   \n",
            "\n",
            "                                 category  \\\n",
            "0                    Science & Technology   \n",
            "1                    Science & Technology   \n",
            "2            Banking & Financial Services   \n",
            "3  Information & Communication Technology   \n",
            "4  Information & Communication Technology   \n",
            "\n",
            "                                      subcategory       type  \\\n",
            "0  Mathematics, Statistics & Information Sciences  Full time   \n",
            "1  Mathematics, Statistics & Information Sciences  Full time   \n",
            "2                               Compliance & Risk  Full time   \n",
            "3                          Engineering - Software  Full time   \n",
            "4                          Developers/Programmers  Full time   \n",
            "\n",
            "                          salary  \n",
            "0                            NaN  \n",
            "1  RM 5,000 – RM 7,000 per month  \n",
            "2                            NaN  \n",
            "3                            NaN  \n",
            "4                            NaN  \n",
            "\n",
            "Original DataFrame info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 606 entries, 0 to 605\n",
            "Data columns (total 10 columns):\n",
            " #   Column        Non-Null Count  Dtype \n",
            "---  ------        --------------  ----- \n",
            " 0   Unnamed: 0    606 non-null    int64 \n",
            " 1   job_id        606 non-null    int64 \n",
            " 2   job_title     606 non-null    object\n",
            " 3   company       606 non-null    object\n",
            " 4   descriptions  588 non-null    object\n",
            " 5   location      606 non-null    object\n",
            " 6   category      606 non-null    object\n",
            " 7   subcategory   606 non-null    object\n",
            " 8   type          606 non-null    object\n",
            " 9   salary        198 non-null    object\n",
            "dtypes: int64(2), object(8)\n",
            "memory usage: 47.5+ KB\n",
            "None\n",
            "\n",
            "Menerapkan cleaning pipeline ke kolom 'job_title'\n",
            "Menerapkan cleaning pipeline ke kolom 'job_description'\n",
            "\n",
            "DataFrame head setelah pembersihan teks (kolom relevan):\n",
            "Error: Kolom \"['cleaned_descriptions'] not in index\" tidak ditemukan di DataFrame. Pastikan nama kolom sudah benar.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if 'df' in locals() and not df.empty:\n",
        "    # Simpan DataFrame yang sudah dibersihkan ke CSV\n",
        "    csv_filename = 'cleaned_job_data.csv'\n",
        "    df.to_csv(csv_filename, index=False)\n",
        "    print(f\"\\nData bersih berhasil disimpan ke file CSV: '{csv_filename}'\")\n",
        "\n",
        "    # Simpan DataFrame yang sudah dibersihkan ke JSON\n",
        "    json_filename = 'cleaned_job_data.json'\n",
        "    df.to_json(json_filename, orient='records', indent=4) # indent=4 agar file mudah dibaca\n",
        "    print(f\"Data bersih berhasil disimpan ke file JSON: '{json_filename}'\")\n",
        "else:\n",
        "    print(\"DataFrame kosong atau tidak ditemukan. Pastikan langkah memuat dan memproses data berhasil.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8eRSSsjtE-S",
        "outputId": "57180f19-32d2-40b2-b044-d12e071896e8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Data bersih berhasil disimpan ke file CSV: 'cleaned_job_data.csv'\n",
            "Data bersih berhasil disimpan ke file JSON: 'cleaned_job_data.json'\n"
          ]
        }
      ]
    }
  ]
}